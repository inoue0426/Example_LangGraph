{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41798190-c31d-458a-8edf-6b762f3e15b5",
   "metadata": {},
   "source": [
    "# Reflexion\n",
    "https://langchain-ai.github.io/langgraph/tutorials/reflexion/reflexion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e9482b-a40f-489e-b6b5-ce9e04126a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b23d30e-1459-48d3-a78b-39298f070901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "\n",
    "search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01db4d7a-a725-4365-a8b1-7cf2c6c5cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import ValidationError\n",
    "# NOTE: you must use langchain-core >= 0.3 with Pydantic v2\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Reflection(BaseModel):\n",
    "    missing: str = Field(description=\"Critique of what is missing.\")\n",
    "    superfluous: str = Field(description=\"Critique of what is superfluous\")\n",
    "\n",
    "\n",
    "class AnswerQuestion(BaseModel):\n",
    "    \"\"\"Answer the question. Provide an answer, reflection, and then follow up with search queries to improve the answer.\"\"\"\n",
    "\n",
    "    answer: str = Field(description=\"~250 word detailed answer to the question.\")\n",
    "    reflection: Reflection = Field(description=\"Your reflection on the initial answer.\")\n",
    "    search_queries: list[str] = Field(\n",
    "        description=\"1-3 search queries for researching improvements to address the critique of your current answer.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ResponderWithRetries:\n",
    "    def __init__(self, runnable, validator):\n",
    "        self.runnable = runnable\n",
    "        self.validator = validator\n",
    "\n",
    "    def respond(self, state: list):\n",
    "        response = []\n",
    "        for attempt in range(3):\n",
    "            response = self.runnable.invoke(\n",
    "                {\"messages\": state}, {\"tags\": [f\"attempt:{attempt}\"]}\n",
    "            )\n",
    "            try:\n",
    "                self.validator.invoke(response)\n",
    "                return response\n",
    "            except ValidationError as e:\n",
    "                state = state + [\n",
    "                    response,\n",
    "                    ToolMessage(\n",
    "                        content=f\"{repr(e)}\\n\\nPay close attention to the function schema.\\n\\n\"\n",
    "                        + self.validator.schema_json()\n",
    "                        + \" Respond by fixing all validation errors.\",\n",
    "                        tool_call_id=response.tool_calls[0][\"id\"],\n",
    "                    ),\n",
    "                ]\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53946596-87ea-4251-8bd0-b13d9acd75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "actor_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are expert researcher.\n",
    "Current time: {time}\n",
    "\n",
    "1. {first_instruction}\n",
    "2. Reflect and critique your answer. Be severe to maximize improvement.\n",
    "3. Recommend search queries to research information and improve your answer.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"\\n\\nReflect on the user's original question and the\"\n",
    "            \" actions taken thus far. Respond using the {function_name} function.\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(\n",
    "    time=lambda: datetime.datetime.now().isoformat(),\n",
    ")\n",
    "initial_answer_chain = actor_prompt_template.partial(\n",
    "    first_instruction=\"Provide a detailed ~250 word answer.\",\n",
    "    function_name=AnswerQuestion.__name__,\n",
    ") | llm.bind_tools(tools=[AnswerQuestion])\n",
    "validator = PydanticToolsParser(tools=[AnswerQuestion])\n",
    "\n",
    "first_responder = ResponderWithRetries(\n",
    "    runnable=initial_answer_chain, validator=validator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814b7919-6c55-4095-a457-7a08f334e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_question = \"Why is reflection useful in AI?\"\n",
    "initial = first_responder.respond([HumanMessage(content=example_question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b50874-0a19-4f08-8f61-c7b2f5e2500e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c128f0f-cef1-407c-9ca2-05a107ecf35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "revise_instructions = \"\"\"Revise your previous answer using the new information.\n",
    "    - You should use the previous critique to add important information to your answer.\n",
    "        - You MUST include numerical citations in your revised answer to ensure it can be verified.\n",
    "        - Add a \"References\" section to the bottom of your answer (which does not count towards the word limit). In form of:\n",
    "            - [1] https://example.com\n",
    "            - [2] https://example.com\n",
    "    - You should use the previous critique to remove superfluous information from your answer and make SURE it is not more than 250 words.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Extend the initial answer schema to include references.\n",
    "# Forcing citation in the model encourages grounded responses\n",
    "class ReviseAnswer(AnswerQuestion):\n",
    "    \"\"\"Revise your original answer to your question. Provide an answer, reflection,\n",
    "\n",
    "    cite your reflection with references, and finally\n",
    "    add search queries to improve the answer.\"\"\"\n",
    "\n",
    "    references: list[str] = Field(\n",
    "        description=\"Citations motivating your updated answer.\"\n",
    "    )\n",
    "\n",
    "\n",
    "revision_chain = actor_prompt_template.partial(\n",
    "    first_instruction=revise_instructions,\n",
    "    function_name=ReviseAnswer.__name__,\n",
    ") | llm.bind_tools(tools=[ReviseAnswer])\n",
    "revision_validator = PydanticToolsParser(tools=[ReviseAnswer])\n",
    "\n",
    "revisor = ResponderWithRetries(runnable=revision_chain, validator=revision_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4046f8d-7258-468f-a59b-7df9812945ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-09-23T00:49:34.377029Z', 'message': {'role': 'assistant', 'content': '', 'tool_calls': [{'function': {'name': 'ReviseAnswer', 'arguments': {'reflection': '{}', 'search_queries': '[\"reflecting on user questions\", \"actions taken so far\"]'}}}]}, 'done_reason': 'stop', 'done': True, 'total_duration': 28840220333, 'load_duration': 48572291, 'prompt_eval_count': 1404, 'prompt_eval_duration': 24479103000, 'eval_count': 36, 'eval_duration': 4281572000}, id='run-b4a62fd0-0f44-482e-b26f-d16d4c026bdf-0', tool_calls=[{'name': 'ReviseAnswer', 'args': {'reflection': '{}', 'search_queries': '[\"reflecting on user questions\", \"actions taken so far\"]'}, 'id': '3f30d767-f729-4205-a043-ebe89af934a8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1404, 'output_tokens': 36, 'total_tokens': 1440})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "revised = revisor.respond(\n",
    "    [\n",
    "        HumanMessage(content=example_question),\n",
    "        initial,\n",
    "        ToolMessage(\n",
    "            tool_call_id=initial.tool_calls[0][\"id\"],\n",
    "            content=json.dumps(\n",
    "                tavily_tool.invoke(\n",
    "                    {\"query\": initial.tool_calls[0][\"args\"][\"search_queries\"][0]}\n",
    "                )\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b10c2-6b38-41a5-bda4-25d73c381c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c306f863-5bb9-4cea-83e9-88b9c1cef4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def run_queries(search_queries: list[str], **kwargs):\n",
    "    \"\"\"Run the generated queries.\"\"\"\n",
    "    return tavily_tool.batch([{\"query\": query} for query in search_queries])\n",
    "\n",
    "\n",
    "tool_node = ToolNode(\n",
    "    [\n",
    "        StructuredTool.from_function(run_queries, name=AnswerQuestion.__name__),\n",
    "        StructuredTool.from_function(run_queries, name=ReviseAnswer.__name__),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d6d7f8-7fee-40bb-a06c-69d971ede038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88674543-e0f4-403c-a416-1efaa7dd4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "MAX_ITERATIONS = 5\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"draft\", first_responder.respond)\n",
    "\n",
    "\n",
    "builder.add_node(\"execute_tools\", tool_node)\n",
    "builder.add_node(\"revise\", revisor.respond)\n",
    "# draft -> execute_tools\n",
    "builder.add_edge(\"draft\", \"execute_tools\")\n",
    "# execute_tools -> revise\n",
    "builder.add_edge(\"execute_tools\", \"revise\")\n",
    "\n",
    "# Define looping logic:\n",
    "\n",
    "\n",
    "def _get_num_iterations(state: list):\n",
    "    i = 0\n",
    "    for m in state[::-1]:\n",
    "        if m.type not in {\"tool\", \"ai\"}:\n",
    "            break\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "def event_loop(state: list) -> Literal[\"execute_tools\", \"__end__\"]:\n",
    "    # in our case, we'll just stop after N plans\n",
    "    num_iterations = _get_num_iterations(state)\n",
    "    if num_iterations > MAX_ITERATIONS:\n",
    "        return END\n",
    "    return \"execute_tools\"\n",
    "\n",
    "\n",
    "# revise -> execute_tools OR end\n",
    "builder.add_conditional_edges(\"revise\", event_loop)\n",
    "builder.add_edge(START, \"draft\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05bd3c-d1d2-495d-9aad-65c59c8a68df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "354bab15-ed1e-422f-9624-62ced51c79c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f0e9b-dc10-4422-b67f-a21f3b48769f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53f82221-6e94-49a6-942a-f2ace5467226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How should we handle the climate crisis?\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "variable messages should be a list of base messages, got {'messages': [HumanMessage(content='How should we handle the climate crisis?', additional_kwargs={}, response_metadata={}, id='f6a3e938-d291-4291-8c9b-83b9b9297885')]} of type <class 'dict'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m events \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow should we handle the climate crisis?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]},\n\u001b[1;32m      3\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mStep \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multi/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1278\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1273\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1274\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1275\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1276\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1277\u001b[0m     ):\n\u001b[0;32m-> 1278\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1285\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/multi/lib/python3.11/site-packages/langgraph/pregel/runner.py:52\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     50\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/miniconda3/envs/multi/lib/python3.11/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/multi/lib/python3.11/site-packages/langgraph/utils/runnable.py:385\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/miniconda3/envs/multi/lib/python3.11/site-packages/langgraph/utils/runnable.py:167\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 167\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m, in \u001b[0;36mResponderWithRetries.respond\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     30\u001b[0m response \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 32\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattempt:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mattempt\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidator\u001b[38;5;241m.\u001b[39minvoke(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/multi/lib/python3.11/site-packages/langchain_core/runnables/base.py:3020\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3018\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3020\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/miniconda3/envs/multi/lib/python3.11/site-packages/langchain_core/prompts/base.py:192\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[1;32m    191\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multi/lib/python3.11/site-packages/langchain_core/runnables/base.py:1923\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1919\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1920\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1921\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1922\u001b[0m         Output,\n\u001b[0;32m-> 1923\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1926\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1929\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1930\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1931\u001b[0m     )\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1933\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/multi/lib/python3.11/site-packages/langchain_core/runnables/config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multi/lib/python3.11/site-packages/langchain_core/prompts/base.py:167\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m    166\u001b[0m     _inner_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(inner_input)\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inner_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multi/lib/python3.11/site-packages/langchain_core/prompts/chat.py:773\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format_prompt\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m    765\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt. Should return a PromptValue.\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \n\u001b[1;32m    767\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;124;03m        PromptValue.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 773\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mmessages)\n",
      "File \u001b[0;32m~/miniconda3/envs/multi/lib/python3.11/site-packages/langchain_core/prompts/chat.py:1216\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend([message_template])\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1214\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[1;32m   1215\u001b[0m ):\n\u001b[0;32m-> 1216\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mmessage_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(message)\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/multi/lib/python3.11/site-packages/langchain_core/prompts/chat.py:239\u001b[0m, in \u001b[0;36mMessagesPlaceholder.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    234\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name, [])\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptional\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name]\n\u001b[1;32m    237\u001b[0m )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be a list of base messages, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m     )\n\u001b[1;32m    243\u001b[0m value \u001b[38;5;241m=\u001b[39m convert_to_messages(value)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_messages:\n",
      "\u001b[0;31mValueError\u001b[0m: variable messages should be a list of base messages, got {'messages': [HumanMessage(content='How should we handle the climate crisis?', additional_kwargs={}, response_metadata={}, id='f6a3e938-d291-4291-8c9b-83b9b9297885')]} of type <class 'dict'>"
     ]
    }
   ],
   "source": [
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", \"How should we handle the climate crisis?\")]},\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for i, step in enumerate(events):\n",
    "    print(f\"Step {i}\")\n",
    "    step['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62476782-a388-4af4-9572-8aa33882a80e",
   "metadata": {},
   "source": [
    "# Looks like the outpu is different from their expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a7c4f-e63a-49ab-84e0-8cef4e59e3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
